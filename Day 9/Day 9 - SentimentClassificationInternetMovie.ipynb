{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f591bde2-868e-4daf-9cd6-49aa83c7fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f6c61ee-3887-4eb1-b7f3-b11554b1964c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sande\\anaconda3\\envs\\local\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f59d9771-9a86-464c-8b12-5c3b4b0a9da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a44cad18-1e48-4026-93b3-f8b0260f0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and Dependencies\n",
    "from fastbook import *\n",
    "from fastai.text.all import *\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf3c217d-af18-4219-b27b-151aa6a51398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastai has a number of Dataset which makes easy to download and to use. Let's use IMDB Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "663579b0-a48f-4bb7-9199-5b347ec7641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and accessing the IMDB Dataset\n",
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d253fb50-bf01-44ae-8a4c-f73202624369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_text_files function is used to grab all the text files in a path obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d77b28-685d-462f-a155-479ba9b572ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrifi'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting all the text Files\n",
    "files = get_text_files(path, folders=[\"train\",\"test\",\"unsup\"])\n",
    "\n",
    "# Inspecting the files\n",
    "text = files[0].open().read()\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69ea6b-f26d-4fce-ac01-d184c9404b82",
   "metadata": {},
   "source": [
    "Word Tokenization:\n",
    "    I have used Fastai Tokenizer for the process of Word Tokenization. Then, I will use Fastai coll_repr function to display the results. It displays the first n items of the collection. The collection of text documents should be wrap into list. The tokens starting with xx are the special tokens which is not a common word prefix in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b766372c-8fed-4844-b14d-18fb117d0ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','xxmaj','aside','from','the','terrific','sea','rescue','sequences',',','of'...]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Word tokenization\n",
    "spacy = WordTokenizer()\n",
    "\n",
    "tokens = Tokenizer(spacy)\n",
    "display(coll_repr(tokens(text), 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a903627-55e3-41b4-97d9-0f2f0ba08185",
   "metadata": {},
   "source": [
    "Subword Tokenization:\n",
    "In Chinese and Japanese languages there are no spaces in the sentences. Similarly Turkish languages add many subwords together without spaces creating very long words. In such problems the Subword tokenization plays the key role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74608565-ab25-4c2f-8634-dca3cdd5e027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'▁O n ce ▁again ▁M r . ▁Co st n er ▁has ▁ d ra g g ed ▁out ▁a ▁movie ▁for ▁far ▁long er ▁than ▁ ne ce s s ar y . ▁A side ▁from ▁the ▁ ter'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subword Tokenization\n",
    "texts = L(x.open().read() for x in files[:2000])\n",
    "\n",
    "def subword(sz):\n",
    "    sp = SubwordTokenizer(vocab_sz=sz)\n",
    "    sp.setup(texts)\n",
    "    return \" \".join(first(sp([text]))[:40])\n",
    "\n",
    "subword(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee08673-6412-4446-8d75-104629470121",
   "metadata": {},
   "source": [
    "Numericalization:\n",
    "Numericalization is the process of mapping tokens to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd6ada01-05f7-40da-998b-77e4aedce4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has'...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#1968) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','a','and','of','to','is','it','i','in','this','\"','that','-',\"'s\",'movie','\\n\\n','was','for','but'...]\n"
     ]
    }
   ],
   "source": [
    "# Numericalization\n",
    "token = tokens(text)\n",
    "token200 = texts[:200].map(tokens)\n",
    "# tokens200 = text[:200].map(lambda x: tokens(x).cpu())\n",
    "display(token200[0])\n",
    "\n",
    "num = Numericalize()\n",
    "num.setup(token200)\n",
    "print(coll_repr(num.vocab, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d21e474-a9b1-48af-9a81-deb485497036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shape of X is torch.Size([64, 72])'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Shape of y is torch.Size([64, 72])'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preparing LMDataLoader\n",
    "nums200 = token200.map(num)\n",
    "dl = LMDataLoader(nums200)\n",
    "\n",
    "# Inspecting the LMDataLoader\n",
    "X, y = first(dl)\n",
    "display(f\"Shape of X is {X.shape}\")\n",
    "display(f\"Shape of y is {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b585e-9df7-4cf5-8b72-1d9166fd424e",
   "metadata": {},
   "source": [
    "### Training the Text Classifier\n",
    "Assembling the Data for training. There are two steps for training the state of art Text classifier using Transfer Learning. First the model should be fine tuned on IMDB reviews corpus on Wikipedia. Then the model can be used to train the classifier.\n",
    "#### Language Model using DataBlock\n",
    "Fastai handles Tokenization and Numericalization automatically when TextBlock is passed to the DataBlock. All the arguments that can be passed to Tokenize and Numericalize can also be passed to the TetxBlock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c9457b9-e7a1-4aff-8e5c-33a2d162f8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj this is one strange movie , from floating images of xxmaj greek statues to flashy images in a</td>\n",
       "      <td>xxmaj this is one strange movie , from floating images of xxmaj greek statues to flashy images in a picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>whose inane , excruciating , nails - on - blackboard screeching is enough to make one wish that xxmaj freddie</td>\n",
       "      <td>inane , excruciating , nails - on - blackboard screeching is enough to make one wish that xxmaj freddie xxmaj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preparing the Language Model using DataBlock.\n",
    "get_imdb = partial(get_text_files, folders=[\"train\", \"test\", \"unsup\"])\n",
    "\n",
    "# Preparing DataBlock.\n",
    "dls_lm = DataBlock(\n",
    "    blocks = TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
    ").dataloaders(path, path=path, bs=32, seq_len=20)\n",
    "\n",
    "# Inspecting the DataBlock.\n",
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68158921",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "306ad96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the Language Model\n",
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, drop_mult=0.3,\n",
    "    metrics = [accuracy, Perplexity()]\n",
    ").to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model = learn.model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38a202bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model = learn.model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "948ddabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.370206</td>\n",
       "      <td>4.230157</td>\n",
       "      <td>0.277683</td>\n",
       "      <td>68.728050</td>\n",
       "      <td>7:15:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training the model\n",
    "learn.fit_one_cycle(1, 2e-2)                    # Training the Model for one Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c664ad",
   "metadata": {},
   "source": [
    "The perplexity metric used here is often used in NLP for language models. It is the exponential of the loss function cross entropy. I have also included accuracy as the metric for the Model Evaluation in predicting the next word. Here, the loss function is cross entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a7078",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "774d0f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('C:/Users/sande/.fastai/data/imdb/models/firstmodel.pth')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the Model trained above\n",
    "learn.save(\"firstmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91146cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.text.learner.LMLearner at 0x23628a4a7d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the model\n",
    "learn.load(\"firstmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faca1eb",
   "metadata": {},
   "source": [
    "Preparing the model: Tuning the final model after unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "317aad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.unfreeze()\n",
    "# learn.fit_one_cycle(6, 2e-3)\n",
    "\n",
    "# Takes time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2eb44",
   "metadata": {},
   "source": [
    "Text Generation: Before moving to fine tuning the Classifier, I will use the Model to generate the random reviews. Since, it is trained to guess the next word of the sentence, I can use the Model to write the new reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ba15ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am bored with the movie because of the actors that were recognized by the television industry . The acting was just what i imagined . The actors were able to get an impression of their roles as Santa Claus ( it seems like a mother ) , and a bit hit and\n",
      "i am bored with the movie because i think it 's a good movie . But the only reason i gave it was that i did n't feel like watching the movie . i watched it and i did n't think it was a good movie . The part i watched was that it was\n",
      "i am bored with the movie because it 's felt like it is a remake of the original Citizen Kane . It is a good movie . It is very worth remembering . At least , it has some potential to be a film from the movies . But the acting\n"
     ]
    }
   ],
   "source": [
    "# Text Generation with Final Model.\n",
    "TEXT = 'I am bored with the movie because'\n",
    "N_words = 50\n",
    "N_sents = 3\n",
    "\n",
    "# Making predictions of the Next Word:\n",
    "preds = [learn.predict(TEXT, N_words, temperature=0.75)\n",
    "         for _ in range(N_sents)]\n",
    "\n",
    "print(\"\\n\".join(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f1c497",
   "metadata": {},
   "source": [
    "Creating the classifier Data Loaders: The Language Model prepared earlier predicts the next word of the Document so it doesn't need any external nabels. However, the classifier predicts external label. In the case of IMDB, it's the sentiment of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70622fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj this movie was recently released on xxup dvd in the xxup us and i finally got the chance to see this hard - to - find gem . xxmaj it even came with original theatrical previews of other xxmaj italian horror classics like \" xxunk \" and \" beyond xxup the xxup darkness \" . xxmaj unfortunately , the previews were the best thing about this movie . \\n\\n \" zombi 3 \" in a bizarre way is actually linked to the infamous xxmaj lucio xxmaj fulci \" zombie \" franchise which began in 1979 . xxmaj similarly compared to \" zombie \" , \" zombi 3 \" consists of a threadbare plot and a handful of extremely bad actors that keeps this ' horror ' trash barely afloat . xxmaj the gore is nearly non - existent ( unless one is frightened of people running around with</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj chris xxmaj rock deserves better than he gives himself in \" down xxmaj to xxmaj earth . \" xxmaj as directed by brothers xxmaj chris &amp; xxmaj paul xxmaj weitz of \" american xxmaj pie \" fame , this uninspired remake of xxmaj warren xxmaj beatty 's 1978 fantasy \" heaven xxmaj can xxmaj wait , \" itself a rehash of 1941 's \" here xxmaj comes xxmaj mr . xxmaj jordan , \" lacks the xxunk profane humor that won xxmaj chris xxmaj rock an xxmaj emmy for his first xxup hbo special . xxmaj predictably , he spouts swear words from a to xxup z , but he consciously avoids the xxmaj f - word . xxmaj anybody who saw this gifted african - american comic in \" lethal xxmaj weapon 4 , \" \" dogma , \" or \" nurse xxmaj betty \" knows he</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preparing the TextBlock and DataBlock of the classifiers\n",
    "dls_clas = DataBlock(\n",
    "    blocks = (TextBlock.from_folder(path, vocab=dls_lm.vocab), CategoryBlock),\n",
    "    get_y = parent_label, \n",
    "    get_items = partial(get_text_files, folders=[\"train\", \"test\"]),\n",
    "    splitter = GrandparentSplitter(valid_name=\"test\")\n",
    ").dataloaders(path, path=path, bs=32, seq_len=20)\n",
    "\n",
    "# Inspecting the DataBlock.\n",
    "dls_clas.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8264eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the Model to classify Texts.\n",
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,\n",
    "                                metrics=accuracy).to_fp16()\n",
    "\n",
    "# Loading the Encoder.\n",
    "# learn.load_encoder(\"firstmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34a86c1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SequentialRNN:\n\tMissing key(s) in state_dict: \"0.module.encoder.weight\", \"0.module.encoder_dp.emb.weight\", \"0.module.rnns.0.weight_hh_l0_raw\", \"0.module.rnns.0.module.weight_ih_l0\", \"0.module.rnns.0.module.bias_ih_l0\", \"0.module.rnns.0.module.bias_hh_l0\", \"0.module.rnns.1.weight_hh_l0_raw\", \"0.module.rnns.1.module.weight_ih_l0\", \"0.module.rnns.1.module.bias_ih_l0\", \"0.module.rnns.1.module.bias_hh_l0\", \"0.module.rnns.2.weight_hh_l0_raw\", \"0.module.rnns.2.module.weight_ih_l0\", \"0.module.rnns.2.module.bias_ih_l0\", \"0.module.rnns.2.module.bias_hh_l0\", \"1.layers.0.0.weight\", \"1.layers.0.0.bias\", \"1.layers.0.0.running_mean\", \"1.layers.0.0.running_var\", \"1.layers.0.2.weight\", \"1.layers.1.0.weight\", \"1.layers.1.0.bias\", \"1.layers.1.0.running_mean\", \"1.layers.1.0.running_var\", \"1.layers.1.2.weight\". \n\tUnexpected key(s) in state_dict: \"0.encoder.weight\", \"0.encoder_dp.emb.weight\", \"0.rnns.0.weight_hh_l0_raw\", \"0.rnns.0.module.weight_ih_l0\", \"0.rnns.0.module.bias_ih_l0\", \"0.rnns.0.module.bias_hh_l0\", \"0.rnns.1.weight_hh_l0_raw\", \"0.rnns.1.module.weight_ih_l0\", \"0.rnns.1.module.bias_ih_l0\", \"0.rnns.1.module.bias_hh_l0\", \"0.rnns.2.weight_hh_l0_raw\", \"0.rnns.2.module.weight_ih_l0\", \"0.rnns.2.module.bias_ih_l0\", \"0.rnns.2.module.bias_hh_l0\", \"1.decoder.weight\", \"1.decoder.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# loading the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfirstmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sande\\anaconda3\\envs\\local\\Lib\\site-packages\\fastai\\text\\learner.py:165\u001b[0m, in \u001b[0;36mTextLearner.load\u001b[1;34m(self, file, with_opt, device, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_opt()\n\u001b[0;32m    164\u001b[0m file \u001b[38;5;241m=\u001b[39m join_path_file(file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir, ext\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 165\u001b[0m \u001b[43mload_model_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sande\\anaconda3\\envs\\local\\Lib\\site-packages\\fastai\\text\\learner.py:93\u001b[0m, in \u001b[0;36mload_model_text\u001b[1;34m(file, model, opt, with_opt, device, strict)\u001b[0m\n\u001b[0;32m     91\u001b[0m hasopt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(state)\u001b[38;5;241m==\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopt\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m     92\u001b[0m model_state \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m hasopt \u001b[38;5;28;01melse\u001b[39;00m state\n\u001b[1;32m---> 93\u001b[0m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_raw_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hasopt \u001b[38;5;129;01mand\u001b[39;00m ifnone(with_opt,\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: opt\u001b[38;5;241m.\u001b[39mload_state_dict(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopt\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\sande\\anaconda3\\envs\\local\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SequentialRNN:\n\tMissing key(s) in state_dict: \"0.module.encoder.weight\", \"0.module.encoder_dp.emb.weight\", \"0.module.rnns.0.weight_hh_l0_raw\", \"0.module.rnns.0.module.weight_ih_l0\", \"0.module.rnns.0.module.bias_ih_l0\", \"0.module.rnns.0.module.bias_hh_l0\", \"0.module.rnns.1.weight_hh_l0_raw\", \"0.module.rnns.1.module.weight_ih_l0\", \"0.module.rnns.1.module.bias_ih_l0\", \"0.module.rnns.1.module.bias_hh_l0\", \"0.module.rnns.2.weight_hh_l0_raw\", \"0.module.rnns.2.module.weight_ih_l0\", \"0.module.rnns.2.module.bias_ih_l0\", \"0.module.rnns.2.module.bias_hh_l0\", \"1.layers.0.0.weight\", \"1.layers.0.0.bias\", \"1.layers.0.0.running_mean\", \"1.layers.0.0.running_var\", \"1.layers.0.2.weight\", \"1.layers.1.0.weight\", \"1.layers.1.0.bias\", \"1.layers.1.0.running_mean\", \"1.layers.1.0.running_var\", \"1.layers.1.2.weight\". \n\tUnexpected key(s) in state_dict: \"0.encoder.weight\", \"0.encoder_dp.emb.weight\", \"0.rnns.0.weight_hh_l0_raw\", \"0.rnns.0.module.weight_ih_l0\", \"0.rnns.0.module.bias_ih_l0\", \"0.rnns.0.module.bias_hh_l0\", \"0.rnns.1.weight_hh_l0_raw\", \"0.rnns.1.module.weight_ih_l0\", \"0.rnns.1.module.bias_ih_l0\", \"0.rnns.1.module.bias_hh_l0\", \"0.rnns.2.weight_hh_l0_raw\", \"0.rnns.2.module.weight_ih_l0\", \"0.rnns.2.module.bias_ih_l0\", \"0.rnns.2.module.bias_hh_l0\", \"1.decoder.weight\", \"1.decoder.bias\". "
     ]
    }
   ],
   "source": [
    "# loading the model\n",
    "learn.load(\"firstmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed352ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training only one Epoch.\n",
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
